{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "\n",
    "**1. Import libraries and load data from database.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\izzit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\izzit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\izzit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import more libraries\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from database\n",
    "\n",
    "engine = create_engine('sqlite:///disasterMessage.db')\n",
    "df = pd.read_sql(\"SELECT * FROM myMessage\", engine)\n",
    "X = df.message.values\n",
    "y = df.drop(columns =['id', 'message', 'genre', 'categories'], axis=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>genre</th>\n",
       "      <th>categories</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>direct</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>direct</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>direct</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message   genre  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...  direct   \n",
       "1   7            Is the Hurricane over or is it not over  direct   \n",
       "2   8                    Looking for someone but no name  direct   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...  direct   \n",
       "4  12  says: west side of Haiti, rest of the country ...  direct   \n",
       "\n",
       "                                          categories  related  request  offer  \\\n",
       "0  related-1;request-0;offer-0;aid_related-0;medi...        1        0      0   \n",
       "1  related-1;request-0;offer-0;aid_related-1;medi...        1        0      0   \n",
       "2  related-1;request-0;offer-0;aid_related-0;medi...        1        0      0   \n",
       "3  related-1;request-1;offer-0;aid_related-1;medi...        1        1      0   \n",
       "4  related-1;request-0;offer-0;aid_related-0;medi...        1        0      0   \n",
       "\n",
       "   aid_related  medical_help  medical_products  ...  aid_centers  \\\n",
       "0            0             0                 0  ...            0   \n",
       "1            1             0                 0  ...            0   \n",
       "2            0             0                 0  ...            0   \n",
       "3            1             0                 1  ...            0   \n",
       "4            0             0                 0  ...            0   \n",
       "\n",
       "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
       "0                     0                0       0      0     0           0   \n",
       "1                     0                1       0      1     0           0   \n",
       "2                     0                0       0      0     0           0   \n",
       "3                     0                0       0      0     0           0   \n",
       "4                     0                0       0      0     0           0   \n",
       "\n",
       "   cold  other_weather  direct_report  \n",
       "0     0              0              0  \n",
       "1     0              0              0  \n",
       "2     0              0              0  \n",
       "3     0              0              0  \n",
       "4     0              0              0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Weather update - a cold front from Cuba that could pass over Haiti'\n",
      " 'Is the Hurricane over or is it not over'\n",
      " 'Looking for someone but no name' ...\n",
      " \"Proshika, operating in Cox's Bazar municipality and 5 other unions, Ramu and Chokoria, assessment, 5 kg rice, 1,5 kg lentils to 700 families.\"\n",
      " 'Some 2,000 women protesting against the conduct of the elections were teargassed as they tried to converge on the local electoral commission offices in the southern oil city of Port Harcourt.'\n",
      " 'A radical shift in thinking came about as a result of this meeting, recognizing that HIV/AIDS is at the core of the humanitarian crisis and identifying the crisis itself as a function of the HIV/AIDS pandemic.']\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['related',\n",
       " 'request',\n",
       " 'offer',\n",
       " 'aid_related',\n",
       " 'medical_help',\n",
       " 'medical_products',\n",
       " 'search_and_rescue',\n",
       " 'security',\n",
       " 'military',\n",
       " 'child_alone',\n",
       " 'water',\n",
       " 'food',\n",
       " 'shelter',\n",
       " 'clothing',\n",
       " 'money',\n",
       " 'missing_people',\n",
       " 'refugees',\n",
       " 'death',\n",
       " 'other_aid',\n",
       " 'infrastructure_related',\n",
       " 'transport',\n",
       " 'buildings',\n",
       " 'electricity',\n",
       " 'tools',\n",
       " 'hospitals',\n",
       " 'shops',\n",
       " 'aid_centers',\n",
       " 'other_infrastructure',\n",
       " 'weather_related',\n",
       " 'floods',\n",
       " 'storm',\n",
       " 'fire',\n",
       " 'earthquake',\n",
       " 'cold',\n",
       " 'other_weather',\n",
       " 'direct_report']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_columns = list(df.columns[4:,])\n",
    "y_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Write a tokenization function to process your text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizes(phrase):\n",
    "\n",
    "    token = word_tokenize(phrase)\n",
    "    lemmatize = WordNetLemmatizer()\n",
    "\n",
    "    tidytoken = []\n",
    "    for t in token:\n",
    "        newtoken = lemmatize.lemmatize(t).lower().strip()\n",
    "        tidytoken.append(newtoken)\n",
    "\n",
    "    return tidytoken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Build a machine learning pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('vect', CountVectorizer(tokenizer=tokenizes)), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', MultiOutputClassifier(RandomForestClassifier\n",
    "                                                   (n_estimators=10, random_state=1, n_jobs=2)))]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Train pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(tokenizer=<function tokenizes at 0x0000011C8A62D160>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier(n_estimators=10,\n",
       "                                                                        n_jobs=2,\n",
       "                                                                        random_state=1)))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Train pipeline\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Test your model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: related \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.35      0.44      1527\n",
      "           1       0.82      0.93      0.87      4984\n",
      "           2       0.30      0.07      0.11        43\n",
      "\n",
      "    accuracy                           0.79      6554\n",
      "   macro avg       0.58      0.45      0.48      6554\n",
      "weighted avg       0.77      0.79      0.77      6554\n",
      "\n",
      "Accuracy 0.7914250839182179\n",
      "\n",
      "Category: request \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93      5462\n",
      "           1       0.80      0.36      0.49      1092\n",
      "\n",
      "    accuracy                           0.88      6554\n",
      "   macro avg       0.84      0.67      0.71      6554\n",
      "weighted avg       0.87      0.88      0.86      6554\n",
      "\n",
      "Accuracy 0.878089716203845\n",
      "\n",
      "Category: offer \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      6516\n",
      "           1       0.00      0.00      0.00        38\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      0.99      0.99      6554\n",
      "\n",
      "Accuracy 0.9942020140372292\n",
      "\n",
      "Category: aid_related \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.88      0.79      3887\n",
      "           1       0.74      0.49      0.59      2667\n",
      "\n",
      "    accuracy                           0.72      6554\n",
      "   macro avg       0.73      0.69      0.69      6554\n",
      "weighted avg       0.73      0.72      0.71      6554\n",
      "\n",
      "Accuracy 0.7221544095209033\n",
      "\n",
      "Category: medical_help \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      6044\n",
      "           1       0.59      0.08      0.14       510\n",
      "\n",
      "    accuracy                           0.92      6554\n",
      "   macro avg       0.76      0.54      0.55      6554\n",
      "weighted avg       0.90      0.92      0.90      6554\n",
      "\n",
      "Accuracy 0.9240158681721087\n",
      "\n",
      "Category: medical_products \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      6219\n",
      "           1       0.87      0.08      0.14       335\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.91      0.54      0.56      6554\n",
      "weighted avg       0.95      0.95      0.93      6554\n",
      "\n",
      "Accuracy 0.9522429050961245\n",
      "\n",
      "Category: search_and_rescue \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6395\n",
      "           1       0.45      0.06      0.10       159\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.71      0.53      0.54      6554\n",
      "weighted avg       0.96      0.98      0.97      6554\n",
      "\n",
      "Accuracy 0.9754348489472078\n",
      "\n",
      "Category: security \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6442\n",
      "           1       0.00      0.00      0.00       112\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.97      0.98      0.97      6554\n",
      "\n",
      "Accuracy 0.9829111992676228\n",
      "\n",
      "Category: military \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      6343\n",
      "           1       0.70      0.07      0.12       211\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.83      0.53      0.55      6554\n",
      "weighted avg       0.96      0.97      0.96      6554\n",
      "\n",
      "Accuracy 0.9690265486725663\n",
      "\n",
      "Category: child_alone \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6554\n",
      "\n",
      "    accuracy                           1.00      6554\n",
      "   macro avg       1.00      1.00      1.00      6554\n",
      "weighted avg       1.00      1.00      1.00      6554\n",
      "\n",
      "Accuracy 1.0\n",
      "\n",
      "Category: water \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      6152\n",
      "           1       0.91      0.23      0.37       402\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.93      0.61      0.67      6554\n",
      "weighted avg       0.95      0.95      0.94      6554\n",
      "\n",
      "Accuracy 0.9513274336283186\n",
      "\n",
      "Category: food \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      5851\n",
      "           1       0.81      0.44      0.57       703\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.87      0.72      0.77      6554\n",
      "weighted avg       0.92      0.93      0.92      6554\n",
      "\n",
      "Accuracy 0.9288983826670736\n",
      "\n",
      "Category: shelter \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      6031\n",
      "           1       0.82      0.23      0.36       523\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.88      0.61      0.66      6554\n",
      "weighted avg       0.93      0.93      0.92      6554\n",
      "\n",
      "Accuracy 0.9346963686298444\n",
      "\n",
      "Category: clothing \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6454\n",
      "           1       0.64      0.07      0.13       100\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.81      0.53      0.56      6554\n",
      "weighted avg       0.98      0.99      0.98      6554\n",
      "\n",
      "Accuracy 0.9851998779371376\n",
      "\n",
      "Category: money \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      6381\n",
      "           1       1.00      0.05      0.09       173\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.99      0.52      0.54      6554\n",
      "weighted avg       0.98      0.97      0.96      6554\n",
      "\n",
      "Accuracy 0.9748245346353372\n",
      "\n",
      "Category: missing_people \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6481\n",
      "           1       0.00      0.00      0.00        73\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.98      0.99      0.98      6554\n",
      "\n",
      "Accuracy 0.9888617638083613\n",
      "\n",
      "Category: refugees \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      6346\n",
      "           1       0.62      0.06      0.11       208\n",
      "\n",
      "    accuracy                           0.97      6554\n",
      "   macro avg       0.79      0.53      0.55      6554\n",
      "weighted avg       0.96      0.97      0.96      6554\n",
      "\n",
      "Accuracy 0.9690265486725663\n",
      "\n",
      "Category: death \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6269\n",
      "           1       0.81      0.12      0.21       285\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.89      0.56      0.60      6554\n",
      "weighted avg       0.96      0.96      0.95      6554\n",
      "\n",
      "Accuracy 0.9606347268843455\n",
      "\n",
      "Category: other_aid \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93      5722\n",
      "           1       0.43      0.02      0.05       832\n",
      "\n",
      "    accuracy                           0.87      6554\n",
      "   macro avg       0.65      0.51      0.49      6554\n",
      "weighted avg       0.82      0.87      0.82      6554\n",
      "\n",
      "Accuracy 0.8719865730851388\n",
      "\n",
      "Category: infrastructure_related \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97      6122\n",
      "           1       0.44      0.01      0.02       432\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.69      0.50      0.49      6554\n",
      "weighted avg       0.90      0.93      0.90      6554\n",
      "\n",
      "Accuracy 0.9339334757400061\n",
      "\n",
      "Category: transport \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6253\n",
      "           1       0.62      0.10      0.17       301\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.79      0.55      0.57      6554\n",
      "weighted avg       0.94      0.96      0.94      6554\n",
      "\n",
      "Accuracy 0.9557522123893806\n",
      "\n",
      "Category: buildings \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6230\n",
      "           1       0.68      0.10      0.18       324\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.82      0.55      0.58      6554\n",
      "weighted avg       0.94      0.95      0.94      6554\n",
      "\n",
      "Accuracy 0.9533109551418981\n",
      "\n",
      "Category: electricity \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6423\n",
      "           1       0.60      0.05      0.09       131\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.79      0.52      0.54      6554\n",
      "weighted avg       0.97      0.98      0.97      6554\n",
      "\n",
      "Accuracy 0.9803173634421727\n",
      "\n",
      "Category: tools \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      6517\n",
      "           1       0.00      0.00      0.00        37\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      0.99      0.99      6554\n",
      "\n",
      "Accuracy 0.9943545926151969\n",
      "\n",
      "Category: hospitals \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6482\n",
      "           1       0.00      0.00      0.00        72\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.98      0.99      0.98      6554\n",
      "\n",
      "Accuracy 0.9890143423863289\n",
      "\n",
      "Category: shops \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6524\n",
      "           1       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           1.00      6554\n",
      "   macro avg       0.50      0.50      0.50      6554\n",
      "weighted avg       0.99      1.00      0.99      6554\n",
      "\n",
      "Accuracy 0.9954226426609704\n",
      "\n",
      "Category: aid_centers \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6473\n",
      "           1       0.00      0.00      0.00        81\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       0.49      0.50      0.50      6554\n",
      "weighted avg       0.98      0.99      0.98      6554\n",
      "\n",
      "Accuracy 0.98764113518462\n",
      "\n",
      "Category: other_infrastructure \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6263\n",
      "           1       0.00      0.00      0.00       291\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.48      0.50      0.49      6554\n",
      "weighted avg       0.91      0.96      0.93      6554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\izzit\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy 0.9552944766554776\n",
      "\n",
      "Category: weather_related \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89      4742\n",
      "           1       0.83      0.50      0.63      1812\n",
      "\n",
      "    accuracy                           0.83      6554\n",
      "   macro avg       0.83      0.73      0.76      6554\n",
      "weighted avg       0.83      0.83      0.82      6554\n",
      "\n",
      "Accuracy 0.8347574000610314\n",
      "\n",
      "Category: floods \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      6030\n",
      "           1       0.87      0.19      0.31       524\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.90      0.59      0.64      6554\n",
      "weighted avg       0.93      0.93      0.91      6554\n",
      "\n",
      "Accuracy 0.9327128471162649\n",
      "\n",
      "Category: storm \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      5965\n",
      "           1       0.71      0.39      0.50       589\n",
      "\n",
      "    accuracy                           0.93      6554\n",
      "   macro avg       0.83      0.69      0.73      6554\n",
      "weighted avg       0.92      0.93      0.92      6554\n",
      "\n",
      "Accuracy 0.9310344827586207\n",
      "\n",
      "Category: fire \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      6495\n",
      "           1       1.00      0.02      0.03        59\n",
      "\n",
      "    accuracy                           0.99      6554\n",
      "   macro avg       1.00      0.51      0.51      6554\n",
      "weighted avg       0.99      0.99      0.99      6554\n",
      "\n",
      "Accuracy 0.9911504424778761\n",
      "\n",
      "Category: earthquake \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      5944\n",
      "           1       0.89      0.61      0.73       610\n",
      "\n",
      "    accuracy                           0.96      6554\n",
      "   macro avg       0.93      0.80      0.85      6554\n",
      "weighted avg       0.95      0.96      0.95      6554\n",
      "\n",
      "Accuracy 0.9569728410131217\n",
      "\n",
      "Category: cold \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6428\n",
      "           1       0.79      0.09      0.16       126\n",
      "\n",
      "    accuracy                           0.98      6554\n",
      "   macro avg       0.88      0.54      0.57      6554\n",
      "weighted avg       0.98      0.98      0.97      6554\n",
      "\n",
      "Accuracy 0.9819957277998169\n",
      "\n",
      "Category: other_weather \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      6206\n",
      "           1       0.40      0.02      0.03       348\n",
      "\n",
      "    accuracy                           0.95      6554\n",
      "   macro avg       0.67      0.51      0.50      6554\n",
      "weighted avg       0.92      0.95      0.92      6554\n",
      "\n",
      "Accuracy 0.9464449191333537\n",
      "\n",
      "Category: direct_report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91      5308\n",
      "           1       0.77      0.28      0.41      1246\n",
      "\n",
      "    accuracy                           0.85      6554\n",
      "   macro avg       0.81      0.63      0.66      6554\n",
      "weighted avg       0.84      0.85      0.82      6554\n",
      "\n",
      "Accuracy 0.8475740006103143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = pipeline.predict(X_test)\n",
    "\n",
    "for i in range(len(y_columns)):\n",
    "    print('Category: {} '.format(y_columns[i]))\n",
    "    print(classification_report(y_test[:, i], pred[:, i]))\n",
    "    print('Accuracy {}\\n'.format(accuracy_score(y_test[:, i], pred[:, i])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Improve your model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['memory', 'steps', 'verbose', 'features', 'clf', 'features__n_jobs', 'features__transformer_list', 'features__transformer_weights', 'features__verbose', 'features__text_pipeline', 'features__starting_verb', 'features__text_pipeline__memory', 'features__text_pipeline__steps', 'features__text_pipeline__verbose', 'features__text_pipeline__vect', 'features__text_pipeline__tfidf', 'features__text_pipeline__vect__analyzer', 'features__text_pipeline__vect__binary', 'features__text_pipeline__vect__decode_error', 'features__text_pipeline__vect__dtype', 'features__text_pipeline__vect__encoding', 'features__text_pipeline__vect__input', 'features__text_pipeline__vect__lowercase', 'features__text_pipeline__vect__max_df', 'features__text_pipeline__vect__max_features', 'features__text_pipeline__vect__min_df', 'features__text_pipeline__vect__ngram_range', 'features__text_pipeline__vect__preprocessor', 'features__text_pipeline__vect__stop_words', 'features__text_pipeline__vect__strip_accents', 'features__text_pipeline__vect__token_pattern', 'features__text_pipeline__vect__tokenizer', 'features__text_pipeline__vect__vocabulary', 'features__text_pipeline__tfidf__norm', 'features__text_pipeline__tfidf__smooth_idf', 'features__text_pipeline__tfidf__sublinear_tf', 'features__text_pipeline__tfidf__use_idf', 'clf__estimator__bootstrap', 'clf__estimator__ccp_alpha', 'clf__estimator__class_weight', 'clf__estimator__criterion', 'clf__estimator__max_depth', 'clf__estimator__max_features', 'clf__estimator__max_leaf_nodes', 'clf__estimator__max_samples', 'clf__estimator__min_impurity_decrease', 'clf__estimator__min_impurity_split', 'clf__estimator__min_samples_leaf', 'clf__estimator__min_samples_split', 'clf__estimator__min_weight_fraction_leaf', 'clf__estimator__n_estimators', 'clf__estimator__n_jobs', 'clf__estimator__oob_score', 'clf__estimator__random_state', 'clf__estimator__verbose', 'clf__estimator__warm_start', 'clf__estimator', 'clf__n_jobs'])\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.232, total= 1.0min\n",
      "[CV] clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.230, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.218, total= 1.0min\n",
      "[CV] clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.217, total=  58.1s\n",
      "[CV] clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.237, total=  56.9s\n",
      "[CV] clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.232, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.223, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.217, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.223, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=2, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.228, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.217, total=  56.5s\n",
      "[CV] clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.219, total=  56.5s\n",
      "[CV] clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.204, total=  57.2s\n",
      "[CV] clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.208, total=  58.5s\n",
      "[CV] clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.214, total=  57.2s\n",
      "[CV] clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.219, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.220, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.208, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.214, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=3, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.226, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.218, total= 1.0min\n",
      "[CV] clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.216, total= 1.0min\n",
      "[CV] clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.206, total= 1.0min\n",
      "[CV] clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.196, total= 1.0min\n",
      "[CV] clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 1), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.211, total= 1.0min\n",
      "[CV] clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.219, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.222, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.215, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.204, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5} \n",
      "[CV]  clf__estimator__min_samples_split=4, features__text_pipeline__vect__ngram_range=(1, 2), features__transformer_weights={'text_pipeline': 1, 'starting_verb': 0.5}, score=0.228, total= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 34.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0 1 2]\n",
      "Confusion Matrix:\n",
      " [[ 545 1016    2]\n",
      " [ 316 4616    5]\n",
      " [   3   43    8]]\n",
      "Accuracy: 0.7886786695148001\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[5376   82]\n",
      " [ 696  400]]\n",
      "Accuracy: 0.8812938663411657\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0]\n",
      "Confusion Matrix:\n",
      " [[6515]]\n",
      "Accuracy: 0.9940494354592615\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[3378  467]\n",
      " [1386 1323]]\n",
      "Accuracy: 0.7172718950259384\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6015   30]\n",
      " [ 474   35]]\n",
      "Accuracy: 0.9231003967043027\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6189   13]\n",
      " [ 322   30]]\n",
      "Accuracy: 0.9488861763808362\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6381    5]\n",
      " [ 155   13]]\n",
      "Accuracy: 0.9755874275251755\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6432    5]\n",
      " [ 114    3]]\n",
      "Accuracy: 0.9818431492218492\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6351   10]\n",
      " [ 180   13]]\n",
      "Accuracy: 0.9710100701861458\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0]\n",
      "Confusion Matrix:\n",
      " [[6554]]\n",
      "Accuracy: 1.0\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6157   20]\n",
      " [ 291   86]]\n",
      "Accuracy: 0.9525480622520598\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[5790   45]\n",
      " [ 517  202]]\n",
      "Accuracy: 0.9142508391821789\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[5941   34]\n",
      " [ 449  130]]\n",
      "Accuracy: 0.9263045468416234\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6446    3]\n",
      " [ 104    1]]\n",
      "Accuracy: 0.983674092157461\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6390    1]\n",
      " [ 157    6]]\n",
      "Accuracy: 0.9758925846811107\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6491    2]\n",
      " [  61    0]]\n",
      "Accuracy: 0.9903875495880379\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6328    7]\n",
      " [ 212    7]]\n",
      "Accuracy: 0.9665852914250839\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6234   24]\n",
      " [ 249   47]]\n",
      "Accuracy: 0.9583460482148306\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[5668   15]\n",
      " [ 845   26]]\n",
      "Accuracy: 0.8687824229478182\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6134    9]\n",
      " [ 411    0]]\n",
      "Accuracy: 0.9359169972535856\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6257    9]\n",
      " [ 262   26]]\n",
      "Accuracy: 0.958651205370766\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6219   14]\n",
      " [ 288   33]]\n",
      "Accuracy: 0.9539212694537686\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6424    1]\n",
      " [ 121    8]]\n",
      "Accuracy: 0.9813854134879463\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0]\n",
      "Confusion Matrix:\n",
      " [[6518]]\n",
      "Accuracy: 0.9945071711931645\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6484    1]\n",
      " [  69    0]]\n",
      "Accuracy: 0.9893194995422643\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0]\n",
      "Confusion Matrix:\n",
      " [[6525]]\n",
      "Accuracy: 0.995575221238938\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6485    1]\n",
      " [  68    0]]\n",
      "Accuracy: 0.989472078120232\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6275    3]\n",
      " [ 275    1]]\n",
      "Accuracy: 0.9575831553249924\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[4570  169]\n",
      " [ 894  921]]\n",
      "Accuracy: 0.8378089716203845\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[5993   29]\n",
      " [ 340  192]]\n",
      "Accuracy: 0.9436985047299359\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[5892   59]\n",
      " [ 390  213]]\n",
      "Accuracy: 0.9314922184925236\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6488    0]\n",
      " [  64    2]]\n",
      "Accuracy: 0.9902349710100702\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[5889   43]\n",
      " [ 252  370]]\n",
      "Accuracy: 0.9549893194995422\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6413   14]\n",
      " [ 102   25]]\n",
      "Accuracy: 0.9823008849557522\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[6218    2]\n",
      " [ 328    6]]\n",
      "Accuracy: 0.9496490692706744\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "Labels: [0 1]\n",
      "Confusion Matrix:\n",
      " [[5184  114]\n",
      " [ 880  376]]\n",
      "Accuracy: 0.8483368935001526\n",
      "\n",
      "Best Parameters: {'clf__estimator__min_samples_split': 2, 'features__text_pipeline__vect__ngram_range': (1, 1), 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n",
      "successful\n"
     ]
    }
   ],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    engine = create_engine('sqlite:///disasterMessage.db')\n",
    "    df = pd.read_sql(\"SELECT * FROM myMessage\", engine)\n",
    "    X = df.message.values\n",
    "    y = df.drop(columns =['id', 'message', 'genre', 'categories'], axis=1).values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "            ('starting_verb', StartingVerbExtractor())\n",
    "        ])),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(n_estimators=10, random_state=1, n_jobs=2)))\n",
    "    ])\n",
    "    parameters = {\n",
    "        'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'clf__estimator__min_samples_split': [2, 3, 4],\n",
    "        'features__transformer_weights': (\n",
    "            {'text_pipeline': 1, 'starting_verb': 0.5},\n",
    "        )\n",
    "    }\n",
    "    print(pipeline.get_params().keys())\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters, verbose=3)\n",
    "    return cv\n",
    "\n",
    "\n",
    "def display_results(cv, y_test, y_pred):\n",
    "    for i in range(len(y_columns)):    \n",
    "        labels = np.unique(y_pred[:, i])\n",
    "        confusion_mat = confusion_matrix(y_test[:, i], y_pred[:, i], labels=labels)\n",
    "        accuracy = (y_pred[:, i] == y_test[:, i]).mean()\n",
    "\n",
    "        print(\"Labels:\", labels)\n",
    "        print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"\\nBest Parameters:\", cv.best_params_)\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    display_results(model, y_test, pred)\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Test your model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
